{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_loader = MNIST(\"/home/cyclops/NEU/CS6220/data/mnist/\")\n",
    "mnist_loader.gz = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = mnist_loader.load_training()\n",
    "timages, tlabels = mnist_loader.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "timages = np.array(timages)\n",
    "tlabels = np.array(tlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.reshape(labels, (-1, 1))\n",
    "tlabels = np.reshape(tlabels, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.from_numpy(images))\n",
    "y = Variable(torch.from_numpy(labels), requires_grad=False)\n",
    "_X = Variable(torch.from_numpy(timages))\n",
    "_y = Variable(torch.from_numpy(tlabels), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.type(torch.LongTensor).squeeze_()\n",
    "_y = _y.type(torch.LongTensor).squeeze_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.type(torch.FloatTensor)\n",
    "_X = _X.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNET(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MNISTNET, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(28 * 28, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "#         self.layer3 = nn.Linear(256, 128)\n",
    "#         self.layer4 = nn.Linear(128, 32)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.sigmoid(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = F.sigmoid(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = F.sigmoid(x)\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MNISTNET()\n",
    "net.cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "loss_op = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.cuda()\n",
    "y = y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyclops/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.337862491607666\n",
      "1 2.3316357135772705\n",
      "2 2.323411464691162\n",
      "3 2.3153765201568604\n",
      "4 2.3083691596984863\n",
      "5 2.3025426864624023\n",
      "6 2.2977633476257324\n",
      "7 2.2938170433044434\n",
      "8 2.2904975414276123\n",
      "9 2.2876343727111816\n",
      "10 2.2850961685180664\n",
      "11 2.2827870845794678\n",
      "12 2.280637741088867\n",
      "13 2.2786014080047607\n",
      "14 2.276644229888916\n",
      "15 2.274742364883423\n",
      "16 2.272878885269165\n",
      "17 2.2710413932800293\n",
      "18 2.269221544265747\n",
      "19 2.2674148082733154\n",
      "20 2.265618085861206\n",
      "21 2.2638258934020996\n",
      "22 2.262038230895996\n",
      "23 2.260254144668579\n",
      "24 2.2584729194641113\n",
      "25 2.2566938400268555\n",
      "26 2.2549166679382324\n",
      "27 2.253139019012451\n",
      "28 2.2513628005981445\n",
      "29 2.249584913253784\n",
      "30 2.247807025909424\n",
      "31 2.2460291385650635\n",
      "32 2.2442519664764404\n",
      "33 2.242475748062134\n",
      "34 2.240699052810669\n",
      "35 2.238922119140625\n",
      "36 2.2371444702148438\n",
      "37 2.2353668212890625\n",
      "38 2.2335877418518066\n",
      "39 2.2318074703216553\n",
      "40 2.230025291442871\n",
      "41 2.2282416820526123\n",
      "42 2.2264554500579834\n",
      "43 2.224668025970459\n",
      "44 2.2228782176971436\n",
      "45 2.2210848331451416\n",
      "46 2.219289541244507\n",
      "47 2.21748948097229\n",
      "48 2.215684652328491\n",
      "49 2.2138748168945312\n",
      "50 2.21205997467041\n",
      "51 2.2102413177490234\n",
      "52 2.2084176540374756\n",
      "53 2.2065889835357666\n",
      "54 2.204756736755371\n",
      "55 2.2029197216033936\n",
      "56 2.201077938079834\n",
      "57 2.1992318630218506\n",
      "58 2.1973791122436523\n",
      "59 2.195521354675293\n",
      "60 2.1936564445495605\n",
      "61 2.1917848587036133\n",
      "62 2.189908027648926\n",
      "63 2.188023328781128\n",
      "64 2.1861321926116943\n",
      "65 2.184235095977783\n",
      "66 2.1823325157165527\n",
      "67 2.180422306060791\n",
      "68 2.1785058975219727\n",
      "69 2.1765828132629395\n",
      "70 2.1746532917022705\n",
      "71 2.1727166175842285\n",
      "72 2.1707730293273926\n",
      "73 2.1688218116760254\n",
      "74 2.1668641567230225\n",
      "75 2.1648993492126465\n",
      "76 2.1629269123077393\n",
      "77 2.160947799682617\n",
      "78 2.1589620113372803\n",
      "79 2.156968593597412\n",
      "80 2.15496826171875\n",
      "81 2.152961015701294\n",
      "82 2.150946617126465\n",
      "83 2.148925304412842\n",
      "84 2.1468968391418457\n",
      "85 2.144860029220581\n",
      "86 2.142815589904785\n",
      "87 2.1407628059387207\n",
      "88 2.1387009620666504\n",
      "89 2.136630058288574\n",
      "90 2.1345510482788086\n",
      "91 2.1324622631073\n",
      "92 2.1303651332855225\n",
      "93 2.12825870513916\n",
      "94 2.12614369392395\n",
      "95 2.1240196228027344\n",
      "96 2.121885299682617\n",
      "97 2.119741678237915\n",
      "98 2.117587089538574\n",
      "99 2.1154227256774902\n",
      "100 2.1132471561431885\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(101):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out = net(X)\n",
    "\n",
    "    loss = loss_op(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(epoch, loss.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = _X.cuda()\n",
    "_y = _y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyclops/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "pred = net(_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 229\n",
       "[torch.cuda.ByteTensor of size () (GPU 0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred.max(1)[1] == _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.1070\n",
       "[torch.cuda.FloatTensor of size () (GPU 0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_op(pred, _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
