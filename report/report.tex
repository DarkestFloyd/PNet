\documentclass{IEEEtran}
\pagenumbering{gobble}

\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{multirow}

\title{\textbf{NL2code}}

% Nischal Mahaveer Chand
% Varun Sundar Rabindranath
% Sai Krishna Karanan
\author{
    Sai Krishna Karanam 
    \texttt{karanam.s@husky.neu.edu}
    \and \\
    Nischal Mahaveer Chand 
    \texttt{mahaveerchand.n@husky.neu.edu}
    \and \\
    Varun Sundar Rabindranath 
    \texttt{rabindranath.v@husky.neu.edu}
}
\date{}

\begin{document}

    \maketitle

    \section{Introduction}
    \blindtext

    \section{Related Word}
    \blindtext

    \section{Dataset}
    \blindtext

    \section{Models}
    Building upon Pengcheng's model, we build 4 models, each encodes the input in a slightly 
    different way. First we start by briefly describing Pengcheng's model.

      \subsection{Pengcheng's model}
      Pengcheng recognized that adding syntax information to the model should give better
      prediction. The model takes as input the raw comment and generates an AST of the 
      corresponding code. The model is an Encoder-Decoder model, with the decoder doing most 
      of the heavy lifting. The encoder comprises of an embedding layer, which produces token
      specific embeddings, that are feed into a Bidirectional LSTM (BiLSTM). The output of this 
      BiLSTM layer is a query embedding, which is passed to the decoder as input. \\

      The decoder is slightly complicated and works in mysterious ways! Lord Voldomort himself
      blessed it with his divine wand to produce a magical black box, that generates
      AST's! \\

      As described, the decoder is already at a state-of-the-art level, and needs no further 
      modifications. All models described hereon use the same decoder architecture as Pengcheng's 
      model.

      \subsection{Basic Concat (BC)}
      Our first model - Basic Concat (BC) model, concatenates the POS tag and phrase tag of the 
      current timestep to the corresponding token embedding; Essentially expanding the input
      from \textit{l} to \textit{(l + 2)} dimensions.

      \subsection{Linear Projection (LP)}
      After experimenting with BC, we decided to embed the POS and Phrase tags in the same way the
      tokens are embedded, then perform a linear project on the new embedding, like in BiLSTM.

      \subsection{Linear Projection Reduced Dimension (LPrd)}
      We noticed that the embedding dimensions for POS embedding and Phrase embedding are very
      large for their relatively small vocabulary sizes (14 and 44 respectively). We decided to 
      reduce the embedding dimensions to 8 and 32.

      \subsection{Raw Query Independent Preprojection (AdvLP)}
      Rather then performing a linear projection on the three embeddings together, we tried to 
      preproject the POS embedding and phrase embedding to create a new embedding, augmentation
      embedding, which we then concatenate with the token embedding and performing another
      linear projection to get the final token embeddings. We hypothesized that this would 
      give us better results as we do try to retail as much information as possible from the
      token embedding. 

    \section{Results}
    \begin{center}
      \begin{tabular}{| c | l || c | c |}
        \hline
        \multicolumn{2}{| c ||}{\multirow{2}{*}{Models}} & \multicolumn{2}{ c |}{Metrics} \\
        \cline{3-4}
        \multicolumn{2}{| c ||}{} & BLUE & Accu. \\
        \hline
        \multicolumn{2}{| c ||}{Pengcheng} & 1 & 1\\
        \multicolumn{2}{| c ||}{Base} & 3 & 1\\
        \hline
        \multirow{4}{*}{NL2code} & BC & 1 & 1\\
        & LP & 9 & 1\\
        & LPrd & 1 & 1\\
        & AdvLP & 1 & 1\\
        \hline  
      \end{tabular}
    \end{center}

    \section{Conclusion}
    \blindtext

    % \bibliographystyle{ieeetran}
    % \bibliography{report}

\end{document}
