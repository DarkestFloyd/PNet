\documentclass{IEEEtran}
\pagenumbering{gobble}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{blindtext}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\graphicspath{ {./images/} }

\title{\textbf{NL2code}}

% Nischal Mahaveer Chand
% Varun Sundar Rabindranath
% Sai Krishna Karanan
\author{
    Sai Krishna Karanam 
    \texttt{karanam.s@husky.neu.edu}
    \and \\
    Nischal Mahaveer Chand 
    \texttt{mahaveerchand.n@husky.neu.edu}
    \and \\
    Varun Sundar Rabindranath 
    \texttt{rabindranath.v@husky.neu.edu}
}
\date{}

\begin{document}

    \maketitle

    \section{Introduction}
    \blindtext

    \section{Related Word}
    \blindtext

    \section{Dataset}
      \subsection{Standard Datasets}
        \blindtext
        \subsubsection{Django}
        \blindtext
        \subsubsection{HS}
        \blindtext
      \subsection{Our dataset}
      \blindtext

    \section{Models}
    We describe four models, each is build upon Pengcheng's model, but encodes the input in a 
    different method. First we start by briefly describing Pengcheng's model.

      \subsection{Pengcheng's model}
      Pengcheng recognized that adding syntax information to the model would give better
      predictions results XXX. His model, follows an encoder-decoder architecture, and takes
      the raw comment as input and generates an Abstract Syntax Tree (AST) of the corresponding 
      code as output. \\
      \hspace*{4mm}The encoder comprises of an embedding layer and a Bidirectional LSTM (BiLSTM)
      layer. It takes a comment as input, embeds each word in the comment to give token embeddings
      TE\textsubscript{t}, for each word $ t $ in the comment. Each TE\textsubscript{t} is 
      sequentially feed into the 
      BiLSTM layer, to produces a Query Embedding (QE) of 128 dimensions. QE is passed to the
      decoder module. \\
      \hspace*{4mm}The decoder uses an Recurrent Neural Network (RNN) to sequentially generate 
      each node of the AST. Each node maps to a timestep in the RNN decoding process and thus, 
      generating the AST can be interpreted as unrolling the RNN. The RNN also maintains an 
      internal state to track the generation process at each timestep.

      \hspace*{4mm}The decoder is slightly complicated and works in mysterious ways! Lord 
      Voldomort himself
      blessed it with his divine wand to produce a magical black box, that generates
      ASTs! \\
      \hspace*{4mm}Pengcheng's tackles the problem with a probabilistic grammar model of
      generating an AST $ y $ given NL description $ x $: $ p(y\vert x) $. The best possible AST 
      $ \hat{y} $ is given by:

      \begin{equation}
        \label{eq:pengcheng}
        \hat{y} = \operatorname*{arg\,max}_y p(y\vert x)
      \end{equation}

      $ \hat{y} $ is then deterministically converted to the corresponding Python code using
      \texttt{astor} XXX.

      \begin{figure}[h]
        \centering
        \includegraphics[width=8cm]{pengcheng.png}
        \caption{Architecture of Pengcheng's model.}
        \label{fig:pengcheng}
      \end{figure}

      \subsection{Our models}
      As described, the decoder is already state-of-the-art XXX, and needs no further 
      modifications. All models described hereon use the same decoder architecture as Pengcheng's 
      model with modifications to the encoder and the input data XXX. All models are trained and 
      tested using the dataset decribed in SECTION. % todo add information about the embeddings

        \subsubsection{Basic Concat (BC)}
        For our first attempt to incorporate syntax information into the encoder, we decided to 
        concatenate (shown as ``:'') the POS and phrase ID of each token XXX to the corresponding 
        token embedding, giving us the Augmented Token Embedding (ATE). The ATE is then feed into 
        a modified BiLSTM
        layer that takes 130 dimension embeddings, rather than the default 128 dimensions. \\

        \hspace*{-3.5mm}TE\textsubscript{t} dimensions: 128 \\
        POS\_ID\textsubscript{t} and Phrase\_ID\textsubscript{t} dimensions: 1 each; total 2 \\
        ATE\textsubscript{t} dimension = 130 \\

        \hspace*{-3.5mm}ATE\textsubscript{t} = \lbrack TE\textsubscript{t} : 
        POS\_ID\textsubscript{t} : Phrase\_ID\textsubscript{t}\rbrack \\
        QE = BiLSTM(ATE) \\

        \begin{figure}[h]
          \centering
          \includegraphics[width=8cm]{bc.png}
          \caption{Architecture of Pengcheng's model.}
          \label{fig:bc}
        \end{figure}

        \subsubsection{Linear Projection (LP)}
        To add some syntactic information over a sequence of tokens, we used an embedding layer for
        POS and phrase tags. The resulting TE, POS embedding (POSE), and Phrase embedding (PhE) are
        then concatenated to produce the ATE; which is a $ (128 * 3) $ dimension vector. We then
        apply a linear projection (using a dense layer with the linear activation function) to 
        give ATE Projected (ATEP) which is passed to BiLSTM. \\

        \hspace*{-3.5mm}TE\textsubscript{t} dimension: 128 \\
        POSE\textsubscript{t} dimension: 128 \\
        PhE\textsubscript{t} dimension: 128 \\
        Dense = (128 * 3) input nodes, 128 output nodes \\ 

        \hspace*{-3.5mm}ATE\textsubscript{t} = [TE\textsubscript{t} : 
        POSE\textsubscript{t} : PhE\textsubscript{t}] \\
        ATEP\textsubscript{t} = Dense(ATE\textsubscript{t}) \\
        QE = BiLSTM(ATEP) \\

        \begin{figure}[h]
          \centering
          \includegraphics[width=8cm]{lp.png}
          \caption{Architecture of Pengcheng's model.}
          \label{fig:lp}
        \end{figure}

        \subsubsection{Linear Projection Reduced Dimension (LPrd)} 
        Subsequently, we noticed that the POS and Phrase vocabulary sizes were relatively smaller
        than token vocabulary size. To avoid redundancy XXX, we reduce the embedding dimensions
        of POSE and PhE to 8 and 32 respectively. The process described in LP is then repeated. \\

        \hspace*{-3.5mm}TE\textsubscript{t} dimension: 128 \\
        POSE\textsubscript{t} dimension: 8 \\
        PhE\textsubscript{t} dimension: 32 \\
        Dense = (128 + 8 + 32) input nodes, 128 output nodes \\ 

        \hspace*{-3.5mm}ATE\textsubscript{t} = [TE\textsubscript{t} : 
        POSE\textsubscript{t} : PhE\textsubscript{t}] \\
        ATEP\textsubscript{t} = Dense(ATE\textsubscript{t}) \\
        QE = BiLSTM(ATEP) \\

        \begin{figure}[h]
          \centering
          \includegraphics[width=8cm]{lprd.png}
          \caption{Architecture of Pengcheng's model.}
          \label{fig:lprd}
        \end{figure}

        \subsubsection{Raw Query Independent Preprojection (AdvLP)}
        Rather than applying one linear projection on ATE, we apply two here, where the first is 
        independent of the input query. POSE and PhE are concatenated and projected to create
        Augmentation Embedding (AE), which is then concatenated with TE and projected to produce
        QE. \\

        \hspace*{-3.5mm}TE\textsubscript{t} dimension: 128 \\
        POSE\textsubscript{t} dimension: 128 \\
        PhE\textsubscript{t} dimension: 128 \\
        Dense = (128 * 2) input nodes, 128 output nodes \\ 

        \hspace*{-3.5mm}AE\textsubscript{t} = Dense([POSE\textsubscript{t} : 
        PhE\textsubscript{t}]) \\
        ATE\textsubscript{t} = [TE\textsubscript{t} : AE\textsubscript{t}] \\
        ATEP\textsubscript{t} = Dense(ATE\textsubscript{t}) \\
        QE = BiLSTM(ATEP) \\

        \begin{figure}[h]
          \centering
          \includegraphics[width=8cm]{advlp.png}
          \caption{Architecture of Pengcheng's model.}
          \label{fig:advlp}
        \end{figure}
      % end of Models

    \blindtext

    \section{Experiments}
    All decoder dimensions and configurations are left untouched and are thus the same as in
    Pengcheng's model. For each of the above models, the embedding sizes are as described Models.
    Each model is run for a maximum of 50 epochs, with early stopping if the validation
    metrics do not change for 10 epochs.

    \section{Results}
    \resizebox{8cm}{!} {
      \begin{tabular}{ | c | c || c | c | }
        \hline
        \multicolumn{2}{ | c ||}{\multirow{2}{*}{Models}} & \multicolumn{2}{ c | }{Metrics} \\
        \cline{3-4}
        \multicolumn{2}{ | c ||}{} & BLUE & Accu. \\
        \hline
        \multicolumn{2}{ | c ||}{Pengcheng} & 84.5 & 71.6 \\
        \multicolumn{2}{ | c ||}{Base} & 73.2 & 67.9 \\
        \hline
        \multirow{4}{*}{NL2code} & BC & 73.6 & 69.4 \\
        & LP & \underline{74.3} & \underline{69.7} \\
        & LPrd & 73.6 & 69.0 \\
        & AdvLP & 73.7 & 69.1 \\
        \hline  
      \end{tabular} }

    \section{Conclusion}
    \blindtext

    % \bibliographystyle{ieeetran}
    % \bibliography{report}

\end{document}
