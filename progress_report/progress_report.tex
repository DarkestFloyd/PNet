\documentclass{IEEEtran}
\pagenumbering{gobble}

\title{\textbf{PNet - ParseNet \\ \large{Progress Report}}}

% Nischal Mahaveer Chand
% Varun Sundar Rabindranath
% Sai Krishna Karanan
\author{
    Sai Krishna Karanam 
    \texttt{karanam.s@husky.neu.edu}
    \and \\
    Nischal Mahaveer Chand 
    \texttt{mahaveerchand.n@husky.neu.edu}
    \and \\
    Varun Sundar Rabindranath 
    \texttt{rabindranath.v@husky.neu.edu}
}
\date{}

\begin{document}

    \maketitle

    \section{Changes}

    We have also changed the datasets used for our task. As mentioned in the 
    Edinburd paper (cite), the Hearthstone and Djanjo datasets, are not generalizable and
    would not be the right data to use for the task (Discussed more elaborately in Section
    X). As a result, we decided to scrape our own data from the repositories mentioned 
    in the Edinburd paper.

    \section{Project Plan}
    Due to the important task of gathering our own data for the task, the data creation and 
    processing phase of our original plan has taken up significant time, resulting in the
    following new plan.
    \begin{enumerate}
        \item Phase 1: \textit{Literature Survey} \hfill [\textbf{Done}]
        \item Phase 2: \textit{Data Preprocessing} \hfill [\textbf{In-progress}]
        \item Phase 3: \textit{Model training} \hfill [\textbf{Future work}]
    \end{enumerate}

    \section{Data}

      \subsection{Problems with existing datasets}HS and DJ are bad. Placeholder

      \subsection{Data gathering} Scrape from web.

      \subsection{Data preprocessing} processing...

    \section{Methods}

    The two methods used:

    \begin{enumerate}
        \item Base paper placeholder
        \item Vanilla seq2seq: To further illustrate the problems with HS and DJ data, we train a 
          vanilla seq2seq model in TensorFlow. Google separately provides a high-level API for 
          the same at google/seq2seq \cite{britz2017}, under the Apache 2.0 license. 
          We run the same model without modification. \\
          As TensorFlow provides the ability to see the training process via TensorBoard, we
          stopped model training when we saw no visible improvement on the dev corpus. The model
          trained was a BiDirectional LSTM model, with attention as described in 
          \cite{bahdanau2014neural}.
    \end{enumerate}

    \section{Experiments}

    DO you even experiment?

    \section{Results}

    End of humanity is near.

    \section{Future Word}
      Our current focus is on following two major tasks:
      \subsection{Adding context to data}
        We plan to add context to the data
      \subsection{Creation of dynamic top-down recursive networks}
        Not sure if we should do this. 

    \bibliographystyle{ieeetran}
    \bibliography{progress_report}
\end{document}
